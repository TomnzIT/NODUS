# ğŸ›¡ï¸ Cybersecurity Control Mapping Tool

This application automatically maps and compares two cybersecurity frameworks (e.g. ISO 27002, TISAX, NIST...) using AI. It assesses coverage between controls and generates gap justifications using a local LLM model (Mistral via Ollama).

---

## ğŸš€ Features

- Upload two Excel files (source & target frameworks)
- AI-powered control matching using sentence-transformers (MiniLM)
- Filterable coverage donut chart
- Heatmap by control category
- LLM-generated justifications with gap analysis (Mistral via Ollama)
- PDF report export (in memory, no temporary files)
- Fully automated launch via Docker + Ollama

---

## âš™ï¸ Requirements

- Docker Desktop (Windows, macOS, or Linux)
- Git (optional, for cloning the repository)
- Internet connection (only for first-time model download)

---

## ğŸ§ª First-time Setup

1. Clone the repository:

   git clone https://github.com/yourusername/cyber-mapping-app.git  
   cd cyber-mapping-app

2. Start Ollama and pull the LLM model:

   docker compose up -d ollama  
   docker exec -it ollama ollama pull mistral

3. Launch the app:

   - On **Windows**: double-click `launch.bat`
   - On **Linux/macOS**: run `./launch.sh` in a terminal

---

## ğŸŒ Access the App

Once running, the app will be available at:

   http://localhost:8501

From the interface, you can:

- Upload your source and target Excel files
- Filter by control category or keyword
- Generate justifications (on demand or in batch)
- Visualize match coverage and heatmap
- Export a PDF report summarizing results

---

## ğŸ“ Project Structure

cyber-mapping-app/  
â”œâ”€â”€ app.py                â†’ Main Streamlit interface  
â”œâ”€â”€ mapping.py            â†’ Control matching & category summary  
â”œâ”€â”€ export_pdf.py         â†’ PDF report generation  
â”œâ”€â”€ llm_utils.py          â†’ LLM API calls to Ollama  
â”œâ”€â”€ requirements.txt      â†’ Python dependencies  
â”œâ”€â”€ Dockerfile            â†’ Streamlit app image  
â”œâ”€â”€ docker-compose.yml    â†’ Compose config (Streamlit + Ollama)  
â”œâ”€â”€ launch.bat            â†’ Windows startup script  
â”œâ”€â”€ launch.sh             â†’ Linux/macOS startup script  
â”œâ”€â”€ LICENSE               â†’ MIT license  
â””â”€â”€ README.md             â†’ This file

---

## ğŸ“¦ LLM Model

This app uses the following local model via Ollama:

   mistral

It will be automatically downloaded the first time using:

   docker exec -it ollama ollama pull mistral

---

## ğŸ›‘ Stop the App

To stop all services:

   docker-compose down

---

## ğŸ“ƒ License

This project is licensed under the MIT License. See the LICENSE file for more details.